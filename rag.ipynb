{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "#os.environ[\"YDC_API_KEY\"] = getpass.getpass()\n",
    "#print(os.environ)\n",
    "\n",
    "\n",
    "\n",
    "#client = ChatGroq(\n",
    "#        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "#    )\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from langchain_community.tools.you import YouSearchTool\n",
    "# from langchain_community.utilities.you import YouSearchAPIWrapper\n",
    "\n",
    "#api_wrapper = YouSearchAPIWrapper(num_web_results=1)\n",
    "#search = YouSearchTool(api_wrapper=api_wrapper)\n",
    "\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "# search.invoke({\"query\": \"What happened at the last wimbledon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "today = datetime.datetime.today().strftime(\"%D\")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "# def modify_messages(messages: list):\n",
    "#     return chat_prompt_template.invoke({\"messages\": messages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_stream(stream):\n",
    "#     for s in stream:\n",
    "#         message = s[\"messages\"][-1]\n",
    "#         if isinstance(message, tuple):\n",
    "#             print(message)\n",
    "#         else:\n",
    "#             message.pretty_print()\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        # Debug print to see what's actually in the stream\n",
    "        # print(\"Stream item:\", s)\n",
    "        \n",
    "        # Check if messages exist\n",
    "        if 'messages' not in s or len(s['messages']) == 0:\n",
    "            print(\"No messages in stream item\")\n",
    "            continue\n",
    "        \n",
    "        message = s['messages'][-1]\n",
    "        \n",
    "        # More detailed type checking and printing\n",
    "        if isinstance(message, tuple):\n",
    "            print(\"Tuple message:\", message)\n",
    "        elif message is None:\n",
    "            print(\"Null message encountered\")\n",
    "        else:\n",
    "            try:\n",
    "                message.pretty_print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error printing message: {e}\")\n",
    "                print(\"Message details:\", message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# def initialize_agent_executor():\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "    # return agent_executor\n",
    "\n",
    "def run_agent_conversation():\n",
    "   \n",
    "    # agent_executor = initialize_agent_executor()\n",
    "    \n",
    "  \n",
    "    config = {\"configurable\": {\"thread_id\": \"14\"}}\n",
    "\n",
    "    # First input\n",
    "    first_inputs = {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Hey, I am bob and I live in PUNE. Is it a good place to enjoy my holidays?\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"First Question:\")\n",
    "    print_stream(agent_executor.stream(first_inputs, config=config, stream_mode=\"values\"))\n",
    "\n",
    "    print(\"\\nSecond Question:\")\n",
    "    # Second input\n",
    "    second_inputs = {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Tell me my name and my place name and about the weather in the place where I live. Please give the data\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print_stream(agent_executor.stream(second_inputs, config=config, stream_mode=\"values\"))\n",
    "\n",
    "\n",
    "# run_agent_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell uses the system_message prompt from another file named system_message.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langgraph.checkpoint.memory import MemorySaver  # an in-memory checkpointer\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from system_message import system_message_function\n",
    "system_message = system_message_function()\n",
    "langgraph_agent_executor  = create_react_agent(\n",
    "       model, tools, state_modifier=system_message, checkpointer=memory\n",
    "   )\n",
    "# langgraph_agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "def langgraph():\n",
    "    \n",
    "   \n",
    " \n",
    "   \n",
    "# with open('system_message.txt', 'r',encoding = 'utf-8') as file:\n",
    "#     system_message_content = file.read()\n",
    "# system_message = \"You are a helpful assistant.\"\n",
    "# This could also be a SystemMessage object\n",
    "# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")\n",
    "\n",
    "# memory = MemorySaver()\n",
    "  \n",
    "\n",
    "   config = {\"configurable\": {\"thread_id\": \"test-thread\"}}\n",
    "   print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"Hi, I'm polly! What are the property details avaialable in pune. I want 3 BHK \")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )[\"messages\"][-1]\n",
    "   )\n",
    "   print(\"---\")\n",
    "   print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\"messages\": [(\"user\", \"Remember my name?\")]}, config\n",
    "    )[\"messages\"][-1]\n",
    "   )\n",
    "   print(\"---\")\n",
    "   print(\n",
    "    langgraph_agent_executor.invoke(\n",
    "        {\"messages\": [(\"user\", \"what was that output again?\")]}, config\n",
    "    )[\"messages\"][-1]\n",
    "   )\n",
    "# langgraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need to work on the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# async for event in agent_executor.astream_events(\n",
    "#     {\"messages\": [HumanMessage(content=\"whats the weather in pune?\")]}, version=\"v1\"\n",
    "# ):\n",
    "#     kind = event[\"event\"]\n",
    "#     if kind == \"on_chain_start\":\n",
    "#         if (\n",
    "#             event[\"name\"] == \"Agent\"\n",
    "#         ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "#             print(\n",
    "#                 f\"Starting agent: {event['name']} with input: {event['data'].get('input')}\"\n",
    "#             )\n",
    "#     elif kind == \"on_chain_end\":\n",
    "#         if (\n",
    "#             event[\"name\"] == \"Agent\"\n",
    "#         ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "#             print()\n",
    "#             print(\"--\")\n",
    "#             print(\n",
    "#                 f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
    "#             )\n",
    "#     if kind == \"on_chat_model_stream\":\n",
    "#         content = event[\"data\"][\"chunk\"].content\n",
    "#         if content:\n",
    "#             # Empty content in the context of OpenAI means\n",
    "#             # that the model is asking for a tool to be invoked.\n",
    "#             # So we only print non-empty content\n",
    "#             print(content, end=\"|\")\n",
    "#     elif kind == \"on_tool_start\":\n",
    "#         print(\"--\")\n",
    "#         print(\n",
    "#             f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "#         )\n",
    "#     elif kind == \"on_tool_end\":\n",
    "#         print(f\"Done tool: {event['name']}\")\n",
    "#         print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "#         print(\"--\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
